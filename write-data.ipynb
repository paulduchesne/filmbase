{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy\n",
    "import pandas\n",
    "import pathlib\n",
    "import pydash\n",
    "import requests\n",
    "import tqdm\n",
    "from wikibaseintegrator import WikibaseIntegrator, datatypes, wbi_login\n",
    "from wikibaseintegrator.models import Reference, References\n",
    "from wikibaseintegrator.wbi_config import config as wbi_config\n",
    "from wikibaseintegrator import wbi_login, WikibaseIntegrator\n",
    "from wikibaseintegrator.wbi_enums import ActionIfExists\n",
    "\n",
    "def value_extract(row, col):\n",
    "\n",
    "    ''' Extract dictionary values. '''\n",
    "\n",
    "    return pydash.get(row[col], 'value')    \n",
    "    \n",
    "def sparql_query(query, service):\n",
    "\n",
    "    ''' Send sparql request, and formulate results into a dataframe. '''\n",
    "\n",
    "    r = requests.get(service, params = {'format': 'json', 'query': query})\n",
    "    data = pydash.get(r.json(), 'results.bindings')\n",
    "    data = pandas.DataFrame.from_dict(data)\n",
    "    for x in data.columns:    \n",
    "        data[x] = data.apply(value_extract, col=x, axis=1)\n",
    "\n",
    "    return data\n",
    "\n",
    "def engineer(row, col, extant, entitytype):\n",
    "\n",
    "    ''' Write entities to wikibase. '''\n",
    "\n",
    "    if col == 'O' and row['type'] != 'wikibase-item':\n",
    "        return ''\n",
    "    else:\n",
    "        name = row[col]\n",
    "        if name in extant:\n",
    "            return pathlib.Path(extant[name]).stem\n",
    "        else:\n",
    "            if entitytype == 'P':\n",
    "                create_property = wbi.property.new(datatype=row['type'])\n",
    "                create_property.labels.set('en', name)\n",
    "                report = create_property.write()\n",
    "\n",
    "                # this is a ridiculous solution, a parsable json report would be great.\n",
    "\n",
    "                ident = [x for x in str(report).split('\\n') if \"_id='P\" in x]\n",
    "                if len(ident) == 1:\n",
    "                    return ident[0].split(\"'\")[1]\n",
    "                else:\n",
    "                    raise Exception(\"Surprise, this method didn't work.\")\n",
    "            else:\n",
    "                create_item = wbi.item.new()\n",
    "                create_item.labels.set('en', name)\n",
    "                report = create_item.write()\n",
    "\n",
    "                # this is a ridiculous solution, a parsable json report would be great.\n",
    "\n",
    "                ident = [x for x in str(report).split('\\n') if \"_id='Q\" in x]\n",
    "                if len(ident) == 1:\n",
    "                    return ident[0].split(\"'\")[1]\n",
    "                else:\n",
    "                    raise Exception(\"Surprise, this method didn't work.\")\n",
    "\n",
    "def extant_entities():\n",
    "\n",
    "    ''' Return dictionary of entities per label. '''\n",
    "\n",
    "    q = sparql_query(\"\"\"\n",
    "        SELECT ?entity ?label WHERE {\n",
    "            ?entity rdfs:label ?label . } \"\"\", \n",
    "        f'https://query.filmbase.wiki/proxy/wdqs/bigdata/namespace/wdq/sparql')\n",
    "\n",
    "    if len(q):\n",
    "        q = q[['entity', 'label']].drop_duplicates(subset='label')\n",
    "        q = q.set_index('label').T.to_dict('records')[0]\n",
    "        return q\n",
    "    else:\n",
    "        return dict()\n",
    "\n",
    "def transfer_column(row):\n",
    "\n",
    "    ''' Transfer values between columns. '''\n",
    "\n",
    "    if row['type'] != 'wikibase-item':\n",
    "        return row['O']\n",
    "    else:\n",
    "        return row['O2']\n",
    "\n",
    "def extant_statements():\n",
    "\n",
    "    ''' Return dataframe of statements. '''\n",
    "\n",
    "    q = sparql_query(\"\"\"\n",
    "        SELECT ?S2 ?P2 ?O2 WHERE {\n",
    "            ?S2 ?P2 ?O2 . } \"\"\", \n",
    "        f'https://query.filmbase.wiki/proxy/wdqs/bigdata/namespace/wdq/sparql')\n",
    "\n",
    "    return q\n",
    "\n",
    "with open(pathlib.Path.home() / 'filmbase_config.json') as conf:\n",
    "    conf = json.load(conf)\n",
    "\n",
    "WDUSER = conf['username']\n",
    "WDPASS = conf['password']\n",
    "wbi_config['MEDIAWIKI_API_URL'] = f\"https://{conf['url']}/w/api.php\"\n",
    "wbi_config['USER_AGENT'] = conf['agent']\n",
    "login_instance = wbi_login.Login(user=WDUSER, password=WDPASS)\n",
    "wbi = WikibaseIntegrator(login=login_instance)\n",
    "\n",
    "model = pandas.read_json(pathlib.Path.cwd() / 'data-model.json').rename(columns={'name':'P'})\n",
    "data = pandas.read_json(pathlib.Path.cwd() / 'dataset.json')\n",
    "data = pandas.merge(data, model, on='P', how='left')\n",
    "data['O'] = data['O'].astype('str').str.strip()\n",
    "\n",
    "undefined = data.copy()\n",
    "undefined = undefined.loc[undefined.type.isin([numpy.nan])]\n",
    "\n",
    "if len(undefined):\n",
    "    raise Exception('Properties present in data which are not defined in Data Model.')\n",
    "\n",
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference property\n",
    "\n",
    "cited_dataframe = pandas.DataFrame(data={'P': ['cited by'], 'type': ['string'], 'O': ['string']})\n",
    "cited_dataframe['P2'] = cited_dataframe.apply(engineer, col='P', extant=extant_entities(), entitytype='P', axis=1)\n",
    "\n",
    "print(len(cited_dataframe))\n",
    "cited_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write properties.\n",
    "\n",
    "build_property = data.copy()\n",
    "build_property = build_property[['P', 'type']].drop_duplicates()\n",
    "build_property['P2'] = build_property.apply(engineer, col='P', extant=extant_entities(), entitytype='P', axis=1)\n",
    "build_property = build_property[['P', 'P2']].drop_duplicates()\n",
    "data = pandas.merge(data, build_property, on=['P'], how='left')\n",
    "\n",
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write objects.\n",
    "\n",
    "builder = data.copy()\n",
    "builder = builder[['O', 'type']].drop_duplicates()\n",
    "builder['O2'] = builder.apply(engineer, col='O', extant=extant_entities(), entitytype='O', axis=1)\n",
    "builder = builder.loc[builder.type.isin(['wikibase-item'])]\n",
    "data = pandas.merge(data, builder, on=['O', 'type'], how='left')\n",
    "data['O2'] = data.apply(transfer_column, axis=1)\n",
    "\n",
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write subjects.\n",
    "\n",
    "builder = data.copy()\n",
    "builder = builder[['S']].drop_duplicates()\n",
    "builder['S2'] = builder.apply(engineer, col='S', extant=extant_entities(), entitytype='S', axis=1)\n",
    "data = pandas.merge(data, builder, on=['S'], how='left')\n",
    "\n",
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write statements. \n",
    "\n",
    "existing = extant_statements()\n",
    "for x in ['https://filmbase.wiki/prop/direct/', 'https://filmbase.wiki/entity/']:\n",
    "    for y in ['S2', 'P2', 'O2']:\n",
    "        existing[y] = existing[y].str.replace(x, '', regex=False)\n",
    "\n",
    "existing['Remove'] = True\n",
    "\n",
    "statements = model.copy()\n",
    "statements = statements[['P']].drop_duplicates()\n",
    "statements = pandas.merge(statements, data, on='P', how='left')\n",
    "statements = pandas.merge(statements, existing, on=['S2', 'P2', 'O2'], how='left')\n",
    "statements = statements.loc[~statements.Remove.isin([True])]\n",
    "\n",
    "cite_id = pathlib.Path(extant_entities()['cited by']).stem\n",
    "for x in tqdm.tqdm(statements.to_dict('records')):\n",
    "\n",
    "    element = wbi.item.get(x['S2'])\n",
    "    if x['type'] == 'string':\n",
    "        claim_references = References()  \n",
    "        claim_reference1 = Reference()\n",
    "        if len(x['R']):\n",
    "            claim_reference1.add(datatypes.String(prop_nr=cite_id, value=x['R']))\n",
    "            claim_references.add(claim_reference1)\n",
    "        element.claims.add(datatypes.String(prop_nr=x['P2'], value=str(x['O']), references=claim_references), action_if_exists=ActionIfExists.APPEND)\n",
    "    elif x['type'] == 'quantity':\n",
    "        claim_references = References()  \n",
    "        claim_reference1 = Reference()\n",
    "        if len(x['R']):\n",
    "            claim_reference1.add(datatypes.String(prop_nr=cite_id, value=x['R']))\n",
    "            claim_references.add(claim_reference1)\n",
    "        element.claims.add(datatypes.Quantity(prop_nr=x['P2'], amount=str(x['O']), references=claim_references), action_if_exists=ActionIfExists.APPEND)\n",
    "    else:\n",
    "        claim_references = References()  \n",
    "        claim_reference1 = Reference()\n",
    "        if len(x['R']):\n",
    "            claim_reference1.add(datatypes.String(prop_nr=cite_id, value=x['R']))\n",
    "            claim_references.add(claim_reference1)\n",
    "        element.claims.add(datatypes.Item(prop_nr=x['P2'], value=str(x['O2']), references=claim_references), action_if_exists=ActionIfExists.APPEND)\n",
    "    element.write()\n",
    "\n",
    "print(len(statements))\n",
    "statements.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
